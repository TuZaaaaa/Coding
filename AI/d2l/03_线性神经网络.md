# 线性神经网络

## 线性回归
> 回归（regression）：为一个/多个自变量和因变量之间关系的建模的一类方法
- 假设 x 和 y 之间的关系是线性的
- 基本元素
  - training set: 数据集
  - sample: 样本
  - target: 目标
  - feature: 特征
  - weight: 权重
  - bias: 偏置（指的是所有特征均为 0 时，预测值为多少）
- 仿射变化：通过加权、对特征进行线性变换，并通过偏置项进行平移
- 线性模型（点积形式）：$\hat{y} = w^Tx + b$
  - 通常使用高维数据集，所以使用线性代数的表示法
- 损失函数：量化拟合程度
  - 如果噪声符合正太分布，平方损失等加入极大似然估计（from CS229）

## 基础优化方法
- 梯度下降
  - 挑选一个随机值 $w_0$
  - 重复迭代参数 t=1,2,3
  - $w_t = W_{t=1}-N*(\partial_t/\partial_{w_{t-1}})$
  - 沿梯度方向将增加损失函数值
  - 学习率：步长的超参数
  - 梯度下降通过不断沿着反梯度方向更新参数求解
- 小批量随机梯度下降
  - 随机采样 b 个样本 $i_1,i_2,...,i_b$ 来近似损失，$1/b*\Sigma_{i \in I_b} t(X_i, y_i, W)$
  - 批量大小：b，超参数
  - 太小：不适合并行最大利用资源
  - 太大：内存消耗浪费计算，eg: 所有样本均相同
  - 小批量随机梯度下降是深度学习默认的求解算法

# Softmax 回归
- 多类别分类模型，使用 Softmax 操作子得到每个类别的置信度
- 对类别进行一位有效编码，$y=[y_1,y_2,...,y_n]^T, y_i = 1 if i=y / 0 otherwise$
- 无校验比例
  - 最大值作为预测：$\hat{y}=argmaxO_i$
  - 需要更置信的识别正确类（大余量），$O_y - O_i >= \Delta(y, i)$
- 校验比例
  - 输出匹配概率（非负，和为1），$\hat{y}=softmax(o), \hat{y_i}=exp(o_i)/\Sigma_k*exp(o_k)$
  - 概率 y 和 $\hat{y}$ 的区别作为损失
- 交叉熵损失：常用来衡量两个概率的区别 $H(p, q) = \Sigma_i - p_i*log(q_i)$
  - 将它作为损失：$l(y, \hat{y}) = - \Sigma_i y_i log_{\hat{y_i}} = -log\hat{y_y}$
  - 梯度是真实概率和预测概率的区别
    - $\partial_{o_i}l(y, \hat{y}) = sofemax(o)_i - y_i$
 
# 损失函数
- L2 Loss: $l(y,y^{'}=1/2*(y-y^{'})^2$
  - 求导时 1/2 和 2 抵消
- L1 Loss: $l(y,y^{'}=|y-y^{'}|$
  - 零点不可导
- Huber's Robust Loss
  -  $l(y,y^{'})=|y-y^{'}|-1/2$  $if |y-y^{'} > 1$
  -  $=1/2*(y-y^{'})^2$ $otherwise$


